#Imports neccessary dependencies
import os
import random
import time

import gym
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim


# Imports all our hyperparameters from the other file
from hyperparams import Hyperparameters as params

# stable_baselines3 have wrappers that simplifies
# the preprocessing a lot, read more about them here:
# https://stable-baselines3.readthedocs.io/en/master/common/atari_wrappers.html
from stable_baselines3.common.atari_wrappers import (
    ClipRewardEnv,
    EpisodicLifeEnv,
    FireResetEnv,
    MaxAndSkipEnv,
    NoopResetEnv,
)
from stable_baselines3.common.buffers import ReplayBuffer #Replaybuffer is used to store and sample transitions


# Creates our gym environment and with all our wrappers.
def make_env(env_id, seed, idx, capture_video, run_name):
    def thunk(): # creation and configuration of environment
        env = gym.make(env_id) # create environment using the given environment ID
        env = gym.wrappers.RecordEpisodeStatistics(env) # wrap environment to record episode statistics
        if capture_video: #records the video if capture_video=True
            if idx == 0:
                env = gym.wrappers.RecordVideo(env, f"videos/{run_name}") # store the videos in a diectory named videos
        env = NoopResetEnv(env, noop_max=30) #wraps environment to perform random value of no-ops at start of episode (max 30)
        env = MaxAndSkipEnv(env, skip=4) #apply max and skip technique
        env = EpisodicLifeEnv(env) #handle episodic life problems in atari games
        if "FIRE" in env.unwrapped.get_action_meanings():
            env = FireResetEnv(env)    #if "FIRE" is availible at start of episode, press it.
        env = ClipRewardEnv(env)    #clip rewards to certain range
        env = gym.wrappers.ResizeObservation(env, (84, 84))    #resize observations to 84x84
        env = gym.wrappers.GrayScaleObservation(env)         #grayscale observations
        env = gym.wrappers.FrameStack(env, 4)              #stack last 4 frames as observations
        env.seed(seed)           #seed for random number generator
        env.action_space.seed(seed)    #seed for random action generator
        env.observation_space.seed(seed) #seed for the environment's observation space random number generator
        return env

    return thunk


class QNetwork(nn.Module):
  def __init__(self, env):
    super().__init__()
    # TODO: Deinfe your network (agent)
    # Look at Section 4.1 in the paper for help: https://arxiv.org/pdf/1312.5602v1.pdf
    self.conv_layers = nn.Sequential(
      nn.Conv2d(4, 16, kernel_size=8, stride=4),  # The first hidden layer convolves 16 8 × 8 filters with stride 4 with the input image
      nn.ReLU(), #applies a rectifier nonlinearity
      nn.Conv2d(16, 32, kernel_size=4, stride=2),  #The second hidden layer convolves 32 4 × 4 filters with stride 2
      nn.ReLU()
    )
    # Calculate the input size for the fully connected layer
    conv_output_size = self._get_conv_output_size(env)
    self.fc_layers = nn.Sequential(
      nn.Linear(conv_output_size, 256),  # Fully connected layer with 256 units
      nn.ReLU(),
      nn.Linear(256, env.single_action_space.n)  # Output layer with n units where n is the number of actions
    )

  def forward(self, x):
    x = x.float()
    x = self.conv_layers(x)
    x = x.view(x.size(0), -1)  # Flatten the output of convolutional layers
    x = self.fc_layers(x)
    return x

  def _get_conv_output_size(self, env):
    # Compute the output size after passing through convolutional layers
    dummy_input = torch.zeros(1, 4, 84, 84)  # Create a dummy input tensor with the specified shape
    dummy_output = self.conv_layers(dummy_input)  # Pass through convolutional layers
    return dummy_output.view(-1).size(0)  # Return the flattened size of the output

def linear_schedule(start_e: float, end_e: float, duration: int, t: int):
    slope = (end_e - start_e) / duration #calculate the slope
    return max(slope * t + start_e, end_e)  #returns the calculated value of e at the given time step t

if __name__ == "__main__": 
    run_name = f"{params.env_id}__{params.exp_name}__{params.seed}__{int(time.time())}" #setup name of the run

    #code to ensure reproducibility by setting seeds to the random number generators
    random.seed(params.seed)
    np.random.seed(params.seed)
    torch.manual_seed(params.seed)
    torch.backends.cudnn.deterministic = params.torch_deterministic
    
    #set device to cuda to speed up processing 
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    if torch.cuda.is_available():
        print(f"CUDA is available. Found {torch.cuda.device_count()} device(s).")
        for i in range(torch.cuda.device_count()):
            print(f"Device {i}: {torch.cuda.get_device_name(i)}")
    else:
        print("CUDA is not available.")

    # environment setup
    envs = gym.vector.SyncVectorEnv([make_env(params.env_id, params.seed, 0, params.capture_video, run_name)])
    assert isinstance(envs.single_action_space, gym.spaces.Discrete), "only discrete action space is supported"

    #initialize network and optimizer
    q_network = QNetwork(envs).to(device)
    optimizer = optim.Adam(q_network.parameters(), lr=params.learning_rate)
    target_network = QNetwork(envs).to(device)
    target_network.load_state_dict(q_network.state_dict())


    # We’ll be using experience replay memory for training our DQN.
    # It stores the transitions that the agent observes, allowing us to reuse this data later.
    # By sampling from it randomly, the transitions that build up a batch are decorrelated.
    # It has been shown that this greatly stabilizes and improves the DQN training procedure.
    rb = ReplayBuffer(
        params.buffer_size,
        envs.single_observation_space,
        envs.single_action_space,
        device,
        optimize_memory_usage=False,
        handle_timeout_termination=True,
    )


    obs = envs.reset() #reset environment and retain initialm observation
    for global_step in range(params.total_timesteps): #loop over timesteps, in our case 10 000 000 :(
        # Here we get epsilon for our epislon greedy.
        epsilon = linear_schedule(params.start_e, params.end_e, params.exploration_fraction * params.total_timesteps, global_step)

        if random.random() < epsilon:
            actions = np.random.randint(0, envs.single_action_space.n, size=(envs.num_envs,)) # TODO: sample a random action from the environment
        #if random < epsilon which has the probability epsilon, then a random action is selected
        else:
            q_values = q_network(torch.tensor(obs).to(device)) # TODO: get q_values from the network you defined, what should the network receive as input?
            actions = torch.argmax(q_values, dim=1).cpu().numpy()
        #otherwise selects action based on q-values predicted by the q-network

        # Take a step in the environment
        next_obs, rewards, dones, infos = envs.step(actions)

        # Here we print our reward.
        for info in infos:
            if "episode" in info.keys():
                print(f"global_step={global_step}, episodic_return={info['episode']['r']}")
                break

        # Save data to replay buffer
        real_next_obs = next_obs.copy()
        for idx, d in enumerate(dones):
            if d:
                real_next_obs[idx] = infos[idx]["terminal_observation"]

        # Here we store the transitions in D
        rb.add(obs, real_next_obs, actions, rewards, dones, infos)

        obs = next_obs
        # Training, starts at 80 000 time steps
        if global_step > params.learning_starts:
            if global_step % params.train_frequency == 0: #only trains every 4 time steps
                # Sample random minibatch of transitions from D
                data = rb.sample(params.batch_size)
                # You can get data with:
                # data.observation, data.rewards, data.dones, data.actions

                with torch.no_grad():
                    # Now we calculate the y_j for non-terminal phi.
                    future_q_values = target_network(data.next_observations)
                    target_max, _ = future_q_values.max(dim=1)# TODO: Calculate max Q
                    #Had to squeeze the rewards and dones to have the right dimensions for loss function
                    rewards = data.rewards.squeeze(dim=1)
                    dones = data.dones.squeeze(dim=1)
                    #calculate targets using the bellman equation
                    td_target = rewards + (params.gamma * target_max * (1 - dones))
                    


                #calculate the loss
                old_val = q_network(data.observations)
                td_target = td_target.unsqueeze(1).expand(-1, old_val.size(1))
                loss = F.mse_loss(old_val, td_target)

                # perform our gradient decent step
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

            # update target network
            if global_step % params.target_network_frequency == 0:
                for target_network_param, q_network_param in zip(target_network.parameters(), q_network.parameters()):
                    target_network_param.data.copy_(
                        params.tau * q_network_param.data + (1.0 - params.tau) * target_network_param.data
                    )

    if params.save_model:
        model_path = f"runs/{run_name}/{params.exp_name}_model"
        torch.save(q_network.state_dict(), model_path)
        print(f"model saved to {model_path}")

    envs.close()
