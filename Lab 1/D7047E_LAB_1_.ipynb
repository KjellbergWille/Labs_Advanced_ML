{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Practical 1: Chatbot – Simple ANN & Transformers\n",
        "\n",
        "## Task 1.1\n",
        "\n",
        "Create a simple chatbot that analyzes text responses typed by a user using an artificial neural network (ANN). The minimum requirement is that the bot prompts a response from the user (with various possible prompts). When the user has typed the answer, the bot should analyze the text (using an ANN) and formulate a response based on the analysis.\n",
        "\n",
        "A small dataset of product reviews that have been labelled as negative (0) or positive (1) is provided in the Files - Exercises - Lab 1 folder, along with some code needed to extract information.\n",
        "\n",
        "A suggested approach is first to try to train a network on the given data. When that task has been concluded, the model can be improved by finding more data, using a dataset with a broader range of labels, using word embeddings to create unique sentence embeddings, making the bot capable of extended dialogue or any other extension you want to pursue.\n",
        "\n",
        "## Task 1.2 Transformers Implementation\n",
        "\n",
        "For this task, you will implement your transformer in PyTorch. You are instructed to follow this link: [Transformers in Pytorch](https://pytorch.org/tutorials/beginner/transformer_tutorial.html).\n",
        "\n",
        "## Task 1.2 (Alternative)\n",
        "\n",
        "If you find any problems with the previous code, links, or set-up (due to Amazon or anything else), we offer you another alternative to developing your own transformer.\n",
        "\n",
        "You can follow Andrej Karpathy's tutorial for a NanoGPT here: [YouTube](https://www.youtube.com/watch?v=kCc8FmEb1nY) and use his GitHub with the code here: [NanoGPT](https://github.com/karpathy/nanoGPT?tab=readme-ov-file) or you can develop your own code if you want.\n",
        "\n",
        "The only requirement is to standardize your training data. Make it the same across your implementations.\n",
        "\n",
        "## Task 1.3 Comparison\n",
        "\n",
        "Here, it would be best if you did a comparison of both models; you are requested to modify your Chatbot to use the same data as the transformer and answer the following:\n",
        "\n",
        "- Compare the performance of the two models and explain in which scenarios you would prefer one over the other.\n",
        "- How did the two models’ complexity, accuracy, and efficiency differ? Did one model outperform the other in specific scenarios or tasks? If so, why?\n",
        "- What insights did you obtain concerning the data amount to train? Embeddings utilized? Architectural choices made?\n"
      ],
      "metadata": {
        "id": "TtXOI3W8wZyp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uXFhNTi7z346"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_pandas(data, columns):\n",
        "    df_ = pd.DataFrame(columns=columns)\n",
        "    data['Sentence'] = data['Sentence'].str.lower()\n",
        "    data['Sentence'] = data['Sentence'].replace('[^\\w\\s]', '', regex=True)\n",
        "    rows = []\n",
        "    for index, row in data.iterrows():\n",
        "        word_tokens = word_tokenize(row['Sentence'])\n",
        "        filtered_sentence = [w for w in word_tokens if not w in stopwords.words('english')]\n",
        "        row_data = {\n",
        "            \"Class\": row.get('Class', None),\n",
        "            \"Sentence\": \" \".join(filtered_sentence)\n",
        "        }\n",
        "        if 'index' in columns:\n",
        "            row_data[\"index\"] = row.get('index', index)\n",
        "        rows.append(row_data)\n",
        "    df_ = pd.concat([df_, pd.DataFrame(rows)], ignore_index=True)\n",
        "    return df_\n",
        "\n",
        "\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(vocab_size, 16)\n",
        "        self.fc2 = nn.Linear(16, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x.squeeze()\n",
        "\n",
        "def chatbot(model, vectorizer):\n",
        "    print(\"Hello! I'm a KLAB sentiment chatbot. Type 'quit' to exit.\")\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == 'quit':\n",
        "            break\n",
        "        processed_input = preprocess_pandas(pd.DataFrame([[user_input, None]], columns=['Sentence', 'Class']), ['Sentence', 'Class'])\n",
        "        input_vector = vectorizer.transform([processed_input['Sentence'].values[0]]).toarray()\n",
        "        input_tensor = torch.tensor(input_vector).float()\n",
        "        output = model(input_tensor)\n",
        "        response = 'Positive' if torch.round(output).item() == 1 else 'Negative'\n",
        "        print(f\"Bot: That seems like a {response} statement.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Mount Google Drive\n",
        "    # Hi KLAB people, please put the data file in your drive \"my drive\". #############################################################################\n",
        "    drive.mount('/content/drive')\n",
        "    # Specify the path to your file in Google Drive\n",
        "    file_path = '/content/drive/My Drive/amazon_cells_labelled.txt'\n",
        "    # Load and preprocess the data\n",
        "    data = pd.read_csv(file_path, delimiter='\\t', header=None)\n",
        "    data.columns = ['Sentence', 'Class']\n",
        "    data['index'] = data.index\n",
        "    columns = ['index', 'Class', 'Sentence']\n",
        "    processed_data = preprocess_pandas(data, columns)\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        processed_data['Sentence'].values,\n",
        "        processed_data['Class'].values.astype('int'),\n",
        "        test_size=0.10,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Vectorize the data\n",
        "    vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    X_train_tfidf = vectorizer.fit_transform(X_train).toarray()\n",
        "    X_test_tfidf = vectorizer.transform(X_test).toarray()\n",
        "\n",
        "    # Convert data to PyTorch tensors\n",
        "    X_train_tensor = torch.tensor(X_train_tfidf).float()\n",
        "    y_train_tensor = torch.tensor(y_train).float()\n",
        "    X_test_tensor = torch.tensor(X_test_tfidf).float()\n",
        "    y_test_tensor = torch.tensor(y_test).float()\n",
        "\n",
        "    # Create TensorDatasets and DataLoaders\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "    train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "    test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    # Initialize the model, loss function, and optimizer\n",
        "    model = TextClassifier(len(vectorizer.vocabulary_))\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Train the model\n",
        "    num_epochs = 10\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}')\n",
        "\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            predicted = torch.round(outputs)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        print(f'Accuracy of the model on the test set: {100 * correct / total}%')\n",
        "\n",
        "    # Start the chatbot\n",
        "    chatbot(model, vectorizer)\n"
      ],
      "metadata": {
        "id": "wkD6jQA2yQaa",
        "outputId": "fdecf1b3-3cdd-4597-f83c-848095e29cbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Epoch 1, Loss: 0.6945637424786886\n",
            "Epoch 2, Loss: 0.6830076495806376\n",
            "Epoch 3, Loss: 0.6744857509930928\n",
            "Epoch 4, Loss: 0.6640441338221232\n",
            "Epoch 5, Loss: 0.6462441007296245\n",
            "Epoch 6, Loss: 0.6294456442197164\n",
            "Epoch 7, Loss: 0.6073550383249918\n",
            "Epoch 8, Loss: 0.5831521391868592\n",
            "Epoch 9, Loss: 0.5555845499038696\n",
            "Epoch 10, Loss: 0.5305933713912964\n",
            "Accuracy of the model on the test set: 76.0%\n",
            "Hello! I'm a KLAB sentiment chatbot. Type 'quit' to exit.\n",
            "You: i am a good boy\n",
            "Bot: That seems like a Positive statement.\n",
            "You: i am not a good boy\n",
            "Bot: That seems like a Positive statement.\n",
            "You: i am a bad boy\n",
            "Bot: That seems like a Negative statement.\n",
            "You: i am not a bad boy\n",
            "Bot: That seems like a Negative statement.\n",
            "You: quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words('english')) #retrieves a set of english stopwords (words that dont have any meaning)\n",
        "    text = text.lower() #convert text to lower case\n",
        "    text = re.sub('[^\\w\\s]', '', text) #removes characters that are non word or non whitespace\n",
        "    word_tokens = word_tokenize(text) #tokenize text into individual words\n",
        "    filtered_sentence = [w for w in word_tokens if not w in stop_words] #filters out stopwords from tokenized words\n",
        "    return \" \".join(filtered_sentence) #returns the filtered words back into a string\n",
        "\n",
        "def load_data(file_path):\n",
        "    data = pd.read_csv(file_path, delimiter='\\t', header=None, names=['Sentence', 'Class'])#reads data from CVS file intp pamdas dataframe\n",
        "    data['Sentence'] = data['Sentence'].apply(preprocess_text)#applies the preprocess text function to each sentence in the data\n",
        "    return data\n",
        "\n",
        "def create_vocab(data):\n",
        "    all_words = set(word_tokenize(\" \".join(data['Sentence']))) #creates a set of all unique words in the dataset\n",
        "    vocab = {word: i+1 for i, word in enumerate(all_words)}#a dictinary vocab is created mapping each word with a unique integer index\n",
        "    vocab['<PAD>'] = 0 #special token <PAD> is added with index 0\n",
        "    return vocab\n",
        "\n",
        "def tokenize_data(data, vocab, max_len):\n",
        "    tokenized_data = [[vocab[word] if word in vocab else 0 for word in word_tokenize(sentence)] for sentence in data]#creates a list tokenized data consisting of lists where each list is a tokenized sentence\n",
        "    padded_data = [sentence + [vocab['<PAD>']] * (max_len - len(sentence)) for sentence in tokenized_data]#adds padding so that all sentences in tokenized data have same length\n",
        "    return padded_data\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=512, nhead=8, num_encoder_layers=3, dropout=0.1):\n",
        "        super(TransformerClassifier, self).__init__()#a transformerclassifier is initialized\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model) #each token in input sequence is mapped to a vector of d_model dimensions\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout)#a simgle encoding layer is defined with nhead number of attention heads\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)# a complete transformer encoder is defined with num_encoder_layers layers\n",
        "        self.fc = nn.Linear(d_model, 1)#fully connected layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.mean(dim=1)  # Pooling\n",
        "        return self.fc(x).squeeze()\n",
        "\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train() #set the model to training mode\n",
        "        total_loss = 0 #initialize the loss for ech epoch\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad() # zero the gradients\n",
        "            outputs = model(inputs) #forward pass / compute predicted outputs by passing inputs to the model\n",
        "            loss = criterion(outputs, labels) #calculate batch loss\n",
        "            loss.backward() #backward pass / comupute gradient of the loss\n",
        "            optimizer.step() #update parameters (weights and biases)\n",
        "            total_loss += loss.item() #total loss for the epoch\n",
        "        print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}') #print average loss for each epoch\n",
        "\n",
        "def evaluate_model(model, test_loader): #evaluate performance on test set\n",
        "    model.eval() #set model to evaluation mode\n",
        "    with torch.no_grad(): #disable gradient calculation\n",
        "        correct = 0  # Initialize the number of correctly predicted samples\n",
        "        total = 0    # Initialize the total number of samples\n",
        "\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)  #forward pass / compute predicted outputs by passing inputs to the model\n",
        "            predicted = torch.round(torch.sigmoid(outputs)) #apply sigmoid activation function to get binary predictions\n",
        "            total += labels.size(0) #total number of samples\n",
        "            correct += (predicted == labels).sum().item() #total number of correctly predicted samples\n",
        "        print(f'Accuracy of the model on the test set: {100 * correct / total}%') #print the accuracy for the test set\n",
        "\n",
        "def chatbot(model, vocab, max_len):\n",
        "    print(\"Hello! I'm a sentiment analysis chatbot. Type 'quit' to exit.\")\n",
        "    while True:\n",
        "        user_input = input(\"You: \") #get user input\n",
        "        if user_input.lower() == 'quit': #check if user wants to quit\n",
        "            break\n",
        "        processed_input = preprocess_text(user_input) #preprocess the user input\n",
        "        tokenized_input = [vocab[word] if word in vocab else 0 for word in word_tokenize(processed_input)] #tokenize the input and convert to indices\n",
        "        padded_input = tokenized_input + [vocab['<PAD>']] * (max_len - len(tokenized_input)) #pad to maximum sequence length\n",
        "        input_tensor = torch.tensor([padded_input]).long() #convert to tensor\n",
        "        output = torch.sigmoid(model(input_tensor)) #pass the input through the model and apply sigmoid\n",
        "        response = 'Positive' if output.item() >= 0.5 else 'Negative' #determine sentiment based on the output\n",
        "        print(f\"Bot: That seems like a {response} statement.\") #prints the prediction\n",
        "\n",
        "def main():\n",
        "    #mount drive to get the dataset and create the vocabulary from the data\n",
        "    drive.mount('/content/drive')\n",
        "    file_path = '/content/drive/My Drive/amazon_cells_labelled.txt'\n",
        "    data = load_data(file_path)\n",
        "    vocab = create_vocab(data)\n",
        "    max_len = max(len(word_tokenize(text)) for text in data['Sentence']) #calculate max length along all sentences\n",
        "\n",
        "    #train/test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        data['Sentence'], data['Class'], test_size=0.10, random_state=42)\n",
        "\n",
        "    #tokenize training and test data\n",
        "    X_train = tokenize_data(X_train, vocab, max_len)\n",
        "    X_test = tokenize_data(X_test, vocab, max_len)\n",
        "\n",
        "    #convert data into pytorch tensors\n",
        "    X_train_tensor = torch.tensor(X_train).long()\n",
        "    y_train_tensor = torch.tensor(y_train.values).float()\n",
        "    X_test_tensor = torch.tensor(X_test).long()\n",
        "    y_test_tensor = torch.tensor(y_test.values).float()\n",
        "\n",
        "    #create pytorch dataloader\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    vocab_size = len(vocab) # set vocab size to the length of vocabulary\n",
        "    model = TransformerClassifier(vocab_size)# initialize the model as a tronsformer classifier\n",
        "    criterion = nn.BCEWithLogitsLoss() # loss function\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0001) #optimizer and learning rate\n",
        "\n",
        "    train_model(model, train_loader, criterion, optimizer,10) #train the model for chosen number of epochs\n",
        "    evaluate_model(model, test_loader) #evaluate the models accuracy on test set\n",
        "    chatbot(model, vocab, 50) #initialize chatbot\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3p7aYXJ4faM",
        "outputId": "731d3fd4-181e-4025-af24-d78abba76891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.8276934663454691\n",
            "Epoch 2, Loss: 0.6907510717709859\n",
            "Epoch 3, Loss: 0.659014097849528\n",
            "Epoch 4, Loss: 0.6437768777211507\n",
            "Epoch 5, Loss: 0.5957281708717346\n",
            "Epoch 6, Loss: 0.5234996656576792\n",
            "Epoch 7, Loss: 0.44813999931017556\n",
            "Epoch 8, Loss: 0.3815574884414673\n",
            "Epoch 9, Loss: 0.3158738334973653\n",
            "Epoch 10, Loss: 0.2908288856347402\n",
            "Accuracy of the model on the test set: 72.0%\n",
            "Hello! I'm a sentiment analysis chatbot. Type 'quit' to exit.\n",
            "You: i am a good boy\n",
            "Bot: That seems like a Positive statement.\n",
            "You: i am a bad boy\n",
            "Bot: That seems like a Negative statement.\n",
            "You: i am not a good boy\n",
            "Bot: That seems like a Positive statement.\n",
            "You: i am not a bad boy\n",
            "Bot: That seems like a Negative statement.\n",
            "You: the man has died\n",
            "Bot: That seems like a Positive statement.\n",
            "You: the man has survived\n",
            "Bot: That seems like a Negative statement.\n",
            "You: death\n",
            "Bot: That seems like a Negative statement.\n",
            "You: life\n",
            "Bot: That seems like a Negative statement.\n",
            "You: good\n",
            "Bot: That seems like a Positive statement.\n",
            "You: bad\n",
            "Bot: That seems like a Negative statement.\n",
            "You: horrible\n",
            "Bot: That seems like a Negative statement.\n",
            "You: awesome\n",
            "Bot: That seems like a Positive statement.\n",
            "You: fuck\n",
            "Bot: That seems like a Negative statement.\n",
            "You: fuck me\n",
            "Bot: That seems like a Negative statement.\n",
            "You: bless you\n",
            "Bot: That seems like a Negative statement.\n",
            "You: kiss me\n",
            "Bot: That seems like a Negative statement.\n",
            "You: love\n",
            "Bot: That seems like a Positive statement.\n",
            "You: hate\n",
            "Bot: That seems like a Positive statement.\n",
            "You: quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1.3\n",
        "\n",
        "We just did the sentimental analasys part. For 1.2 we imported the data from task 1.1.\n",
        "\n",
        "1. The first model (ANN) performed better on the test set. However, a transformer-based architecture is more suitable for complex data that is more context-heavy and would be a better choice in that case. ANNs might be more suitable if the task is as simple as segment analysis and the context of the data is less sequential.\n",
        "\n",
        "2. It took alot longer to run the secound model compared to the first, this is due to its more complex structure. When it comes to the complexity it was easier to implement task 1.1, note however that we processed the dataset for 1.2 in a less complex way compared to task 1.1.\n",
        "\n",
        "3. The amount of data is not enough for our inputs, it almost feels random when we enter our own data."
      ],
      "metadata": {
        "id": "zaTjbd8c6F9v"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O7oLZZN5A_Qj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}